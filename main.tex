% !TEX root = main.tex
\input{include.tex}
\usepackage{ctex}
\usepackage{pifont}
\usepackage{cleveref}
\usepackage{pstricks}
\usepackage{pgfplots}
% \usepackage[utf8]{inputenc}
\input{crefchn}
\begin{document}

\input{titlepage.tex}

我们假定读者已经学习或正在学习《高等数学》、《离散数学》、《算法导论》的课程. 其中, 学习并熟练理解高等数学中的概念是至关重要的. 

\part{概率论中基本的概念}
\input{text/ch1/1-intro.tex}
\input{text/ch1/2-prob-evt.tex}
\begin{shaded}
    \input{aside/2star-counting-techique.tex}
\end{shaded}
\input{text/ch1/3-cond-prob.tex}
\input{text/ch1/4-indep.tex}
\begin{shaded}
    \input{aside/1-dependent-formal.tex}
\end{shaded}

\part{一维随机变量}
\input{text/ch2/1-rv.tex}
\input{text/ch2/2-discrete-rv.tex}
\input{text/ch2/3-cont-rv.tex}
\input{text/ch2/4-derived.tex}

\part{多维随机变量及其分布}
\input{text/ch3/1-2drv.tex}
\input{text/ch3/2-margin.tex}
\input{text/ch3/3-dependent.tex}
\input{text/ch3/4-rv-func.tex}

\part{随机变量的数字特征}
\input{text/ch4/1-expectation.tex}
\input{text/ch4/2-cond-expectation.tex}
\input{text/ch4/3-expectation-ineq.tex}
\input{exercise/ch4/exp.tex}
\input{text/ch4/4-vari-intro.tex}
\input{text/ch4/5-cov-mom.tex}
\input{text/ch4/6-median-mean.tex}

\part{概率的极限定理}
\input{text/ch5/1-cov-seqs.tex}


\section{弱大数定律}

\begin{theorem}[Chebyshev大数律] 
    设 $X_1, X_2, \cdots$ 是相互独立的随机变量序列, $\mathrm{E}\left(X_i\right)=\mu_i, \operatorname{var}\left(X_i\right)=\sigma_i^2(i \geqslant 1)$ 且 $\left\{\sigma_i^2, i \geqslant 1\right\}$ 有界, 设 $S_n=\sum_{i=1}^n X_i$ $(n \geqslant 1)$, 则
$$
\frac{S_n-\mathrm{E}\left(S_n\right)}{n} \stackrel{P}{\longrightarrow} 0 \quad(n \rightarrow \infty) .
$$
\end{theorem}

\begin{proof}
    设 $\sigma_i^2 \leqslant M($ 一切 $i \geqslant 1$ ). 利用Chebyshev不等式知
$$
\begin{aligned}
P\left(\left|\frac{S_n-\mathrm{E}\left(S_n\right)}{n}\right| \geqslant \varepsilon\right) & =P\left(\left|S_n-\mathrm{E}\left(S_n\right)\right| \geqslant n \varepsilon\right) \\
& \leqslant \frac{1}{n^2 \varepsilon^2} \operatorname{Var}\left(S_n\right) .
\end{aligned}
$$

由于 $X_1, \cdots, X_n$ 两两不相关, $\operatorname{Var}\left(S_n\right)=\sum_{i=1}^n \operatorname{Var}\left(X_i\right) \leqslant n M$. 于是
$$
P\left(\left|\frac{S_n-\mathrm{E}\left(S_n\right)}{n}\right| \geqslant \varepsilon\right) \leqslant \frac{M}{n \varepsilon^2} \quad(\text { 一切 } \varepsilon>0) .
$$
由此得证. 

\end{proof}

我们就立刻可以得到这样的一个有用的推论: 

\begin{corollary}
    设 $X_1, X_2, \cdots$ 是相互独立同分布的随机变量序列, $\mu=\mathrm{E}\left(X_1\right)$ 和 $\sigma^2=\operatorname{var}\left(X_1\right)$ 都存在, $S_n=\sum_{i=1}^n X_i(n \geqslant 1)$, 则
    $$
    \frac{S_n}{n} \stackrel{P}{\longrightarrow} \mu \quad(n \rightarrow \infty) .
    $$
\end{corollary}

这是我们最常用到的大数律, 其中方差存在的条件可以去掉, 而且可以证明更强的结论, 但证明较复杂, 我们在这里就不证明了. 

实际上, 这个形式最初有Bernouli在做二项分布的时候首次提出. 他注意到了如下的事实: 

\begin{corollary}
    设单次试验中事件 $A$ 发生的概率是 $p$, 在 $n$ 次独立试验 $(n \geqslant 2)$ 中 $A$ 发生了 $\nu_n$ 次, 则
$$
\frac{\nu_n}{n} \stackrel{P}{\longrightarrow} p(n \rightarrow \infty) .
$$
\end{corollary}

\begin{proof}令
$$
X_i= \begin{cases}1, & \text { 第 } i \text { 次试验中 } A \text { 发生, } \\ 0, & \text { 第 } i \text { 次试验中 } A \text { 不发生 }\end{cases}
$$
$(i=1,2, \cdots)$, 则 $\frac{\nu_n}{n}=\frac{1}{n} \sum_{i=1}^n X_i$. 由于 $X_1, X_2, \cdots$ 是相互独立同分布的随机变量序列, $\mathrm{E}\left(X_i\right)=p, \operatorname{var}\left(X_i\right)=p(1-p)(i \geqslant 1)$. 因此成立.
\end{proof}



\end{document}


