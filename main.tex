% !TEX root = main.tex
\input{include.tex}
\usepackage{ctex}
\usepackage{pifont}
\usepackage{cleveref}
% \usepackage[utf8]{inputenc}
\input{crefchn}
\begin{document}

\input{titlepage.tex}

\part{概率论中基本的概念}
\input{text/ch1/1-intro.tex}
\begin{shaded}
    \input{aside/1-things-algebra.tex}
\end{shaded}
\input{text/ch1/2-prob-evt.tex}
\begin{shaded}
    \input{aside/2star-counting-techique.tex}
\end{shaded}
\input{text/ch1/3-cond-prob.tex}
\input{text/ch1/4-indep.tex}
\begin{shaded}
    \input{aside/1-dependent-formal.tex}
\end{shaded}

\part{一维随机变量}
\input{text/ch2/1-rv.tex}
\input{text/ch2/2-discrete-rv.tex}
\input{text/ch2/3-cont-rv.tex}
\input{text/ch2/4-derived.tex}

\part{多维随机变量及其分布}
\input{text/ch3/1-2drv.tex}
\input{text/ch3/2-margin.tex}
\input{text/ch3/3-dependent.tex}
\input{text/ch3/4-rv-func.tex}

\part{随机变量的数字特征}

\section{数学期望}

有这样的一个游戏: 花2元并投掷一颗均匀的骰子. 如果事件A = \{1, 2, 3\} 发生, 收到1元. 如果事件B = \{4, 5\} 发生, 收到2元. 如果事件C = \{6\} 发生, 收到6元. 你会参加这个游戏吗?

\begin{webaside}
    实际上, 真理元素的频道主实际上真的在路边做了这个实验. 可以参看他们的视频: \href{https://www.bilibili.com/video/BV1Xx411b7rM}{Bilibili: BV1Xx411b7rM}
\end{webaside}

可能我们的第一考虑是看看``平均''能得多少. 这样的随机变量$X$, \[
    X =
    \begin{cases}
    1 & \text{如果事件 A 发生} \\
    2 & \text{如果事件 B 发生} \\
    6 & \text{如果事件 C 发生}
    \end{cases}
    \]
    
    事件A、B、C的概率分别是：
    \[P(A) = \frac{3}{6}, \quad P(B) = \frac{2}{6}, \quad P(C) = \frac{1}{6}\]
    
    想法: 求结果的平均值$1 \cdot P(A) + 2 \cdot P(B) + 6 \cdot P(C)= \frac{13}{6}$.

    \begin{definition}[离散型随机变量的期望]
        设离散型随机变量 $X$ 的分布律为
        $$
        P\left\{X=x_k\right\}=p_k, \quad k=1,2, \cdots .
        $$
        若级数
    $$
    \sum_{k=1}^{\infty} x_k p_k
    $$
    
    绝对收敛, 则称级数 $\sum_{k=1}^{\infty} x_k p_k$ 的和为随机变量 $X$ 的数学期望, 记为 $\Ep{X}$. 即
    $$
    \Ep{X}=\sum_{k=1}^{\infty} x_k p_k
    $$
    \end{definition}

    
    仿照离散型随机变量的期望的定义, 我们同样给出连续形随机变量的定义:
    \begin{definition}{连续型随机变量的期望}
        设连续型随机变量 $X$ 的概率密度为 $f(x)$, 若积分
$$
\int_{-\infty}^{\infty} x f(x) \mathrm{d} x
$$

绝对收敛, 则称积分 $\int_{-\infty}^{\infty} x f(x) \mathrm{d} x$ 的值为随机变量 $X$ 的数学期望, 记为 $\Ep{X}$.即
$$
\Ep{X}=\int_{-\infty}^{\infty} x f(x) \mathrm{d} x
$$
    \end{definition}

    下面我们看若干常见分布的数学期望. 

    \subsection{常见分布的数学期望}

    \paragraph{Bernouli分布} 它描述的是只先进行一次事件试验，该事件发生的概率为$p$，不发生的概率为$1-p$. 也就是
    \begin{itemize}
        \item $P(X=1)=p$
        \item $P(X=0)=1-p$
    \end{itemize}

    因此, $ \Ep{X}=0 \cdot(1-p)+1 \cdot p=p$.

    \paragraph{Poisson分布} 它的分布是: 对于大于0的参数$\lambda$, $$
    P\{X=k\}=\frac{\lambda^k e^{-\lambda}}{k !}. 
    $$    

    期望的话, 就是$\sum_{i=0}^{\infty} \frac{i e^{-\lambda} \lambda^i}{i !}$. 我们可以使用《高等数学II》中的公式计算它: 
    $$
\begin{aligned}
{E}(X) & =\lambda e^{-\lambda} \sum_{k \geq 1} \frac{1}{(k-1) !} \lambda^{k-1} & & \text { 去掉 } k=0 \text { 的那一项 } \\
& =\lambda e^{-\lambda} \sum_{j \geq 0} \frac{\lambda^j}{j !} & & \text { 变量代换 } j:=k-1 \\
& =\lambda e^{-\lambda} e^\lambda & & \text {指数函数的 Taylor 级数展开 } \\
& =\lambda & &
\end{aligned}
$$

\paragraph{几何分布}
回忆: 对与$G(X=k)$的几何分布, 表达的意思是前$k-1$次皆失败，第$k$次成功的概率。因此, 其概率是
    $$
    P\{X=k\}=(1-p)^{k-1} p
    $$

    所以它的期望表达式是: 
    $$
    \Ep{X}=\sum_{k=1}^{\infty} k p(1-p)^{k-1}
    $$

    由于$p$是常数, 我们可以把它从求和号中拿出来, 得到
    $p \sum_{k=1}^{\infty} k(1-p)^{k-1}$


\begin{itemize}
    \item 使用: $\frac{1}{1-x}  =1+x+x^2+\cdots =\sum_{k=0}^{\infty} x^k$, 当$|x|<1$时: 
    \item 两端求导: $\frac{1}{(1-x)^2}=\sum_{k=1}^{\infty} k x^{k-1}$.
    \item 令$x=1-p$, 得到$\frac{1}{p^2}=\sum_{k=1}^{\infty} k(1-p)^{k-1}$
    \item 因此期望为$1/p$.
\end{itemize}

实际上, 这个操作正是对应着它的无记忆性. 在介绍了期望的性质之后, 我们发现使用无记忆性来说明这个性质就不用如此大费周章了. 

\paragraph{均匀分布} 回顾: 均匀分布的概率密度函数$$
f(x)= \begin{cases}\frac{1}{b-a} & \text { for } a \leq x \leq b \\ 0 & \text { for } x<a \text { or } x>b\end{cases}
$$
带入计算之后, 得到了一个很平凡的结论: $$\int_a^b \frac{x}{b-a} d x=\frac{a+b}{2}.$$
这也符合直觉 -- 如果你往这里加一个支点的话, 它就会支撑起整个概率密度函数. 

\paragraph{正态分布}
回忆: 正态分布的概率密度函数$$
    f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
    $$

    这个问题, 可以根据对称性进行求解. 注意到关于$\mu$对称.  
    $$
\frac{1}{\sqrt{2 \pi \sigma^2}} \int_{-\infty}^{\infty} x e^{-(x-\mu)^2 / 2 \sigma^2} d x=\mu
$$


    \begin{shaded}
        \paragraph{*二项分布} 它描述的是在$n$次独立重复的Bernoili试验中，设每次试验中事件A发生的概率为$p$。用$X$表示$n$重伯努利试验中事件A发生的{\red{次数}}.

        其概率概率分布为$$P(X=k)={n\choose k} p^k q^{n-k}.$$
        因此
    $$
\begin{aligned}
\Ep{x}= & \teal{0} \cdot {n \choose 0} p^0(1-p)^n+\teal{1} \cdot\binom n1 p^1(1-p)^{n-1}+\cdots \\
& +\teal{(n-1)}\binom n{n-1} p^{n-1}(1-p)^1  +\teal{n}\binom nn
 p^n(1-p)^0 \\
= & \sum_{i=0}^n \teal{i}\binom ni p^i(1-p)^{n-i}
\end{aligned}
$$

我们发现它难以计算. 一个原因是里面那个青色的$i$太难受了. 如果我们能够把它去掉就好了. 幸运的是, 组合恒等式里面有这样的一条性质, 可以帮助我们完成这件事. 

回忆: 二项式系数的定义是 $\binom nk = \frac{n(n-1) \cdots(n-k+1)}{k(k-1) \cdots(1)}=\frac{n !}{k !(n-k) !}$. 其有一个有趣的等式, 可以帮助我们把东西吸收/提取出来. 也就是: 
$$
\binom rk=\frac{r}{k}\binom{r-1}{k-1} \text {, 整数 } k \neq 0
$$
这个一般形象地称为``吸收-提取恒等式''. 这个证明可以使用其定义展开即可. 
$$
\begin{aligned}
\binom rk & =\frac{r !}{k !(r-k) !} \\
& =\frac{r}{k} \cdot \frac{(r-1) !}{(k-1) !(r-k) !} \\
& =\frac{r}{k} \cdot \frac{(r-1) !}{(k-1) !((r-1)-(k-1)) !} \\
& =\frac{r}{k} \cdot \binom {r-1}{k-1}
\end{aligned}
$$


好. 有了这样的一个想法,  根据吸收-提取恒等式:
$$
\begin{aligned}
& \sum_{i=1}^n i\binom ni p^i(1-p)^{n-i} \\
= & \sum_{i=1}^n n\binom{n-1}{i-1} p^i(1-p)^{n-i} \\
= & n p \teal{\sum_{i=1}^n\binom{n-1}{i-1} p^{i-1}(1-p)^{n-i}} \\
= & n p\teal{(p+(1-p))^{n-1}}=n p
\end{aligned}
$$

这个为什么如此简单? 实际上因为期望有一些性质. 我们来看一看期望的若干性质. 
    \end{shaded}

    \subsection{数学期望的重要性质}
    
    \paragraph{1.(常数的数学期望)} 设$C$是常数，则有$\Ep C = C.$

    事实上, 直观来看, 每一次我们得到的都一定是$C$. 因此有其合理性. 

    \paragraph{2. (常数进出数学期望)} 设$C$是常数, $X$是一个随机变量，则有
    \[
        \Ep {CX} = C\Ep X.
    \]
    
    这是因为常数可以自由进出求和号/积分号. 

    \paragraph{3. (两个随机变量之和)}设$X,Y$是两个随机变量，则有
    \[
        E\left(X+Y\right) = E\left(X\right) + E\left(Y\right).
    \]

    事实上, 这是因为分配率得到的结果. 设二维随机变量$\left(X,Y\right)$的概率密度为$f_{X,Y}\left(x,y\right)$,其边缘概率密度为$f_X(x),f_Y(y)$
    \begin{align}
        E\left(X+Y\right) &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\left(x+y\right)f_{X,Y}\left(x,y\right){d}x{d}y \notag\\
        & =  \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}xf_{X,Y}(x,y){d}x{d}y + \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}yf_{X,Y}(x,y){d}x{d}y \notag\\
        & = E\left(X\right) + E\left(Y\right)\notag.
    \end{align}

    上面的$2^\circ$和$3^\circ$合在一起被称作期望的线性性质. 这和我们在线性代数中的线性空间的理论中的一部分很相似. 

    \paragraph{4. (两个\red{独立}的随机变量之积)} 设$X,Y$是相互独立的随机变量，则有\[
        E\left(XY\right) = E\left(X\right)E\left(Y\right).
    \]



    这是因为只有独立的情况下, 联合随机变量的内容才可以得到拆开. 

    若$X$和$Y$相互独立，
        
        \begin{align}
            E\left(XY\right) &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}xyf_{X,Y}\left(x,y\right){d}x{d}y = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}xyf_X\left(x\right)f_Y\left(y\right){d}x{d}y\notag\\
            & = \left[\int_{-\infty}^{\infty}xf_X\left(x\right){d}x\right]\left[\int_{-\infty}^{\infty}yf_Y\left(y\right){d}y\right] = E\left(X\right)E\left(Y\right).\notag
        \end{align}




\end{document}


