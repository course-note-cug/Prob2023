\section{数学期望}

有这样的一个游戏: 花2元并投掷一颗均匀的骰子. 如果事件A = \{1, 2, 3\} 发生, 收到1元. 如果事件B = \{4, 5\} 发生, 收到2元. 如果事件C = \{6\} 发生, 收到6元. 你会参加这个游戏吗?

\begin{webaside}
    实际上, 真理元素的频道主实际上真的在路边做了这个实验. 可以参看他们的视频: \href{https://www.bilibili.com/video/BV1Xx411b7rM}{Bilibili: BV1Xx411b7rM}
\end{webaside}

可能我们的第一考虑是看看``平均''能得多少. 这样的随机变量$X$, \[
    X =
    \begin{cases}
    1 & \text{如果事件 A 发生} \\
    2 & \text{如果事件 B 发生} \\
    6 & \text{如果事件 C 发生}
    \end{cases}
    \]
    
    事件A、B、C的概率分别是：
    \[P(A) = \frac{3}{6}, \quad P(B) = \frac{2}{6}, \quad P(C) = \frac{1}{6}\]
    
    那么, 求结果的平均值$1 \cdot P(A) + 2 \cdot P(B) + 6 \cdot P(C)= \frac{13}{6}$.

    刚刚我们做的事情, 用更正式的语言, 实际上是就是有了一个 $(\Omega, \mathscr{A}, \mathbf{P})$ 这样的有限概率空间, 而 $X=X(\omega)$ 是某一随机变量, 其值域为 $\left\{x_1, \cdots, x_k\right\}$. 如果设 $A_i=\left\{\omega: X(\omega)=x_i\right\}$, 则显然 $X$ 可以表示为
    \mn{\small 下面的歌词阐述了我们在做选择的时候内心的权衡: ``可万一对了呢 ~会不会 ~我会不会更快乐 ~可万一错了呢 ~这一切 ~我是否可以承受''\\--《万一对了呢》 ChiliChill}
    $$
    X(\omega)=\sum_{i=1}^k x_i I\left(A_i\right)
    $$记 $p_i=\mathbf{P}\left\{X=x_i\right\}$. 直观上显然: 如果在 $n$ 次独立重复试验中观测随机变量 $X$的取值, 则取 $x_i$ 的值大致应该出现 $n p_i(i=1, \cdots, k)$ 次. 
    因此, 根据 $n$ 次试验的结果, 计算的该随机变量的 ``平均值'' 大致为
    $$
    \frac{1}{n}\left[n p_1 x_1+\cdots+n p_k x_k\right]=\sum_{i=1}^k p_i x_i
    $$

    \begin{definition}[离散型随机变量的期望]
        设离散型随机变量 $X$ 的分布律为
        $$
        P\left\{X=x_k\right\}=p_k, \quad k=1,2, \cdots .
        $$
        若级数
    $$
    \sum_{k=1}^{\infty} x_k p_k
    $$
    
    绝对收敛, 则称级数 $\sum_{k=1}^{\infty} x_k p_k$ 的和为随机变量 $X$ 的数学期望, 记为 $\Ep{X}$. 即
    $$
    \Ep{X}=\sum_{k=1}^{\infty} x_k p_k
    $$
    \end{definition}

    
    仿照离散型随机变量的期望的定义, 我们同样给出连续形随机变量的定义:
    \begin{definition}{连续型随机变量的期望}
        设连续型随机变量 $X$ 的概率密度为 $f(x)$, 若积分
$$
\int_{-\infty}^{\infty} x f(x) \mathrm{d} x
$$

绝对收敛, 则称积分 $\int_{-\infty}^{\infty} x f(x) \mathrm{d} x$ 的值为随机变量 $X$ 的数学期望, 记为 $\Ep{X}$.即
$$
\Ep{X}=\int_{-\infty}^{\infty} x f(x) \mathrm{d} x
$$
    \end{definition}

    实际上, 连续型随机变量的数学期望有另一种理解. 看下面的例子: 
    \begin{example}
        \label{eg:expectation-calc}
        连续性随机变量$X\geq 0$, 其概率密度函数为$f_X(x)$. 证明$\Ep{X}=\int_0^\infty P(X>t) dt$. 
    \end{example}

    证明直接展开交换积分次序即可. 
    
    下面我们看若干常见分布的数学期望. 

    \subsection{常见分布的数学期望}

    \paragraph{Bernouli分布} 它描述的是只先进行一次事件试验, 该事件发生的概率为$p$, 不发生的概率为$1-p$. 也就是
    \begin{itemize}
        \item $P(X=1)=p$
        \item $P(X=0)=1-p$
    \end{itemize}

    因此, $ \Ep{X}=0 \cdot(1-p)+1 \cdot p=p$.

    \paragraph{Poisson分布} 它的分布是: 对于大于0的参数$\lambda$, $$
    P\{X=k\}=\frac{\lambda^k e^{-\lambda}}{k !}. 
    $$    

    期望的话, 就是$\sum_{i=0}^{\infty} \frac{i e^{-\lambda} \lambda^i}{i !}$. 我们可以使用《高等数学II》中的公式计算它: 
    $$
\begin{aligned}
{E}(X) & =\lambda e^{-\lambda} \sum_{k \geq 1} \frac{1}{(k-1) !} \lambda^{k-1} & & \text { 去掉 } k=0 \text { 的那一项 } \\
& =\lambda e^{-\lambda} \sum_{j \geq 0} \frac{\lambda^j}{j !} & & \text { 变量代换 } j:=k-1 \\
& =\lambda e^{-\lambda} e^\lambda & & \text {指数函数的 Taylor 级数展开 } \\
& =\lambda & &
\end{aligned}
$$

\paragraph{几何分布}
回忆: 对与$G(X=k)$的几何分布, 表达的意思是前$k-1$次皆失败, 第$k$次成功的概率. 因此, 其概率是
    $$
    P\{X=k\}=(1-p)^{k-1} p
    $$

    所以它的期望表达式是: 
    $$
    \Ep{X}=\sum_{k=1}^{\infty} k p(1-p)^{k-1}
    $$

    由于$p$是常数, 我们可以把它从求和号中拿出来, 得到
    $p \sum_{k=1}^{\infty} k(1-p)^{k-1}$


\begin{itemize}
    \item 使用: $\frac{1}{1-x}  =1+x+x^2+\cdots =\sum_{k=0}^{\infty} x^k$, 当$|x|<1$时: 
    \item 两端求导: $\frac{1}{(1-x)^2}=\sum_{k=1}^{\infty} k x^{k-1}$.
    \item 令$x=1-p$, 得到$\frac{1}{p^2}=\sum_{k=1}^{\infty} k(1-p)^{k-1}$
    \item 因此期望为$1/p$.
\end{itemize}

实际上, 这个操作正是对应着它的无记忆性. 在介绍了期望的性质之后, 我们发现使用无记忆性来说明这个性质就不用如此大费周章了. 

\paragraph{均匀分布} 回顾: 均匀分布的概率密度函数$$
f(x)= \begin{cases}\frac{1}{b-a} & \text { for } a \leq x \leq b \\ 0 & \text { for } x<a \text { or } x>b\end{cases}
$$
带入计算之后, 得到了一个很平凡的结论: $$\int_a^b \frac{x}{b-a} d x=\frac{a+b}{2}.$$
这也符合直觉 -- 如果你往这里加一个支点的话, 它就会支撑起整个概率密度函数. 

\paragraph{正态分布}
回忆: 正态分布的概率密度函数$$
    f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
    $$

    这个问题, 可以根据对称性进行求解. 注意到关于$\mu$对称.  
    $$
\frac{1}{\sqrt{2 \pi \sigma^2}} \int_{-\infty}^{\infty} x e^{-(x-\mu)^2 / 2 \sigma^2} d x=\mu
$$


    \begin{shaded}
        \paragraph{*二项分布} 它描述的是在$n$次独立重复的Bernoili试验中, 设每次试验中事件A发生的概率为$p$. 用$X$表示$n$重伯努利试验中事件A发生的{\red{次数}}.
        \mn{像这样对二项式的操作在计算机科学中是很有必要了解的. 因为我们总是在分析算法的时候用到他们. }
        其概率概率分布为$$P(X=k)={n\choose k} p^k q^{n-k}.$$
        因此
    $$
\begin{aligned}
\Ep{x}= & \teal{0} \cdot {n \choose 0} p^0(1-p)^n+\teal{1} \cdot\binom n1 p^1(1-p)^{n-1}+\cdots \\
& +\teal{(n-1)}\binom n{n-1} p^{n-1}(1-p)^1  +\teal{n}\binom nn
 p^n(1-p)^0 \\
= & \sum_{i=0}^n \teal{i}\binom ni p^i(1-p)^{n-i}
\end{aligned}
$$

我们发现它难以计算. 一个原因是里面那个青色的$i$太难受了. 如果我们能够把它去掉就好了. 幸运的是, 组合恒等式里面有这样的一条性质, 可以帮助我们完成这件事. 

回忆: 二项式系数的定义是 $\binom nk = \frac{n(n-1) \cdots(n-k+1)}{k(k-1) \cdots(1)}=\frac{n !}{k !(n-k) !}$. 其有一个有趣的等式, 可以帮助我们把东西吸收/提取出来. 也就是: 
$$
\binom rk=\frac{r}{k}\binom{r-1}{k-1} \text {, 整数 } k \neq 0
$$
这个一般形象地称为``吸收-提取恒等式''. 这个证明可以使用其定义展开即可. 
$$
\begin{aligned}
\binom rk & =\frac{r !}{k !(r-k) !} \\
& =\frac{r}{k} \cdot \frac{(r-1) !}{(k-1) !(r-k) !} \\
& =\frac{r}{k} \cdot \frac{(r-1) !}{(k-1) !((r-1)-(k-1)) !} \\
& =\frac{r}{k} \cdot \binom {r-1}{k-1}
\end{aligned}
$$


好. 有了这样的一个想法,  根据吸收-提取恒等式:
$$
\begin{aligned}
& \sum_{i=1}^n i\binom ni p^i(1-p)^{n-i} \\
= & \sum_{i=1}^n n\binom{n-1}{i-1} p^i(1-p)^{n-i} \\
= & n p \teal{\sum_{i=1}^n\binom{n-1}{i-1} p^{i-1}(1-p)^{n-i}} \\
= & n p\teal{(p+(1-p))^{n-1}}=n p
\end{aligned}
$$

这个为什么结果如此简单? 实际上因为期望有一些性质. 我们下一节会提到原因. 
    \end{shaded}

    有些分布没有数学期望. 比如Cauchy分布: $f_X(x)=\frac{4}{\pi} \frac{4}{1+x^2},-\infty<x<+\infty$. 我们在求它的积分的时候, 就会发现
    $$
\begin{aligned}
& \frac{4}{\pi} \int_{-\infty}^{\infty}|x| \frac{4}{1+x^2} \\
= & \int_0^{+\infty} \frac{2 x}{1+x^2} d x \\
= & \int_0^{+\infty} \frac{1}{1+y} d y=\ln (1+y)<+\infty .
\end{aligned}
$$发散. 因此Cauchy分布没有数学期望. 

    \subsection{数学期望的重要性质}
    
    \paragraph{1.(常数的数学期望)} 设$C$是常数, 则有$\Ep C = C.$

    事实上, 直观来看, 每一次我们得到的都一定是$C$. 因此有其合理性. 

    \paragraph{2. (常数进出数学期望)} 设$C$是常数, $X$是一个随机变量, 则有
    \[
        \Ep {CX} = C\Ep X.
    \]
    
    这是因为常数可以自由进出求和号/积分号. 

    \paragraph{3. (两个随机变量之和)}设$X,Y$是两个随机变量, 则有
    \[
        E\left(X+Y\right) = E\left(X\right) + E\left(Y\right).
    \]

    事实上, 这是因为分配率得到的结果. 设二维随机变量$\left(X,Y\right)$的概率密度为$f_{X,Y}\left(x,y\right)$,其边缘概率密度为$f_X(x),f_Y(y)$
    \begin{align}
        E\left(X+Y\right) &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\left(x+y\right)f_{X,Y}\left(x,y\right){d}x{d}y \notag\\
        & =  \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}xf_{X,Y}(x,y){d}x{d}y + \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}yf_{X,Y}(x,y){d}x{d}y \notag\\
        & = E\left(X\right) + E\left(Y\right)\notag.
    \end{align}

    上面的$2^\circ$和$3^\circ$合在一起被称作期望的线性性质. 这和我们在线性代数中的线性空间的理论中的一部分很相似. 

    \paragraph{4. (两个\red{独立}的随机变量之积)} 设$X,Y$是相互独立的随机变量, 则有\[
        E\left(XY\right) = E\left(X\right)E\left(Y\right).
    \]



    这是因为只有独立的情况下, 联合随机变量的内容才可以得到拆开. 

    若$X$和$Y$相互独立, 
        
        \begin{align}
            E\left(XY\right) &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}xyf_{X,Y}\left(x,y\right){d}x{d}y = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}xyf_X\left(x\right)f_Y\left(y\right){d}x{d}y\notag\\
            & = \left[\int_{-\infty}^{\infty}xf_X\left(x\right){d}x\right]\left[\int_{-\infty}^{\infty}yf_Y\left(y\right){d}y\right] = E\left(X\right)E\left(Y\right).\notag
        \end{align}

    对于上面的二项分布, 我们可以注意到每一次实验, 我们都可以看做一个随机变量. 每一次的期望都是$p$, 自然, 所有的实验, 期望就是$np$. 

    \begin{exercise}
        $A, B, C, D$四人竞拍, 价高者得. 假设你是$A$, 已知$B, C, D$三人竞拍价互相独立, 切服从$U(7,11)$. 如$A$中标可以以10转让, 如何报价使得$A$获得的期望收益最大?
    \end{exercise}

    \begin{solution}
        以$G(A)$表达$A$的收益. $P(G(X)=10-X)=((x-7)/4)^3$, $P(G(X)=0)=1-((x-7)/4)^3$. 那么$\Ep{G(X)}=(10-x)(\frac{x-7} 4)^3$. 得到$x$的极值为$37/4$. 
    \end{solution}
    


    \subsection{随机变量函数的数学期望}
    下面我们来研究随机变量函数的数学期望. 

    \paragraph{1. 由随机变量$X$求$\Ep{f(X)}$} 设 $g(x)$ 是连续函数, $X$ 是随机变量.
    \begin{itemize}
        \item 如果$X$是离散型随机变量, 且分布律为 $P\left(X=x_k\right)=p_k(k=1,2, \cdots)$, 若 $\sum_k g\left(x_k\right) p_k$ 绝对收敛, 则
        $$
        E[g(X)]=\sum_k g\left(x_k\right) p_k .
        $$
        \item 如果$X$是连续型随机变量,且概率密度函数为$f(x)$, 若$\int_{-\infty}^{+\infty} g(x)f(x)\dd x$ 绝对收敛, 则
        $$\int_{-\infty}^{+\infty} g(x)f(x)\dd x.$$
    \end{itemize}
    
    也就是只要把``系数''变为了对应的函数值. 事实上, 在对应的离散的状态下, 我们有如下的说明: 
    \begin{align*} & p_Y(y)=\sum_{\{x \mid f(x)=y\}} p_X(x) \\ & E[f(X)]=E[Y] \\ & =\sum_y y p_Y(y) \\ & =\sum_y y \sum_{\{x \mid f(x)=y\}} p_X(x) \\ & =\sum_y \sum_{\{x \mid f(x)=y\}} y p_X(x) \\ & =\sum_y \sum_{\{x \mid f(x)=y\}} f(x) p_X(x) \\ & =\sum_x f(x) p_X(x)\end{align*}

    \paragraph{2. 用$(X,Y)$得到$\Ep{g(X,Y)}$}设$g(x,y)$为连续函数, 

    离散情形: 
    \begin{itemize}
        \item 若$(X,Y)$为离散型随机变量, 分布律为$p_{ij} = P(X=x_i,Y=y_j)(i,j=1,2,\cdots)$
        \item 且级数$\sum_i \sum_j g(x_i,y_j)p_{ij}$绝对收敛
        \item 则$$\Ep{g(X,Y)}=\sum_i \sum_j g(x_i,y_j)p_{ij}.$$
    \end{itemize}    

    连续情形:\begin{itemize}
        \item 若$(X,Y)$为连续型随机变量, 联合概率密度函数为$f(x,y)$
        \item 且积分$\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}g(x,y)f(x,y)\dd x\dd y$绝对收敛
        \item 则$$\Ep{g(X,Y)}=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}g(x,y)f(x,y)\dd x\dd y.$$
    \end{itemize}  

\subsection*{多知道一点: 快速排序的期望运行时间}

我们在《算法导论》的课程上了解了如下的随机快速排序算法. 使用自然语言大概可以看做\cref{algo:quick-sort}. 
\begin{algorithm}
    \caption{随机快速排序算法}
    \label{algo:quick-sort}
    \KwIn{待排序的数组$S=[x_1, x_2, \cdots, x_n]$}
    \KwOut{排序后的数组$S$.}
    \begin{itemize}
        \item 如果$S$只有一个或者零个元素, 返回$S$. 否则继续. 
        \item 随机选择$S$中的元素$s$作为基准元素. 
        \item 把$S$分为两个小的列表$S_1, S_2$. 其中, 任何一个$S_1$中的元素比$s$要小, 任何一个$S_2$中的元素比$s$要大.
        \item 对$S_1, S_2$进行快速排序. 
        \item 返回列表$[S_1, x, S_2]$.
    \end{itemize}
\end{algorithm}

我们声称: 每一次从元素中独立地随机选取基准, 那么对于任意的输入, 快速排序比较的期望次数为$2n\lg n + O(n)$. 

\begin{proof}
    设$y_1, y_2, \cdots, y_n$是输入值$x_1, x_2, \cdots, x_n$按照升序排列的结果. 我们定义$X_{ij}(i<j)$是一个随机变量. 如果在算法执行的某一时刻$y_i$和$y_j$发生了比较, $X_{ij}$取值为1, 否则为0. 那么比较的总次数满足
    $$
    X=\sum_{i=1}^{n-1}\sum_{j=i+1}^n X_{ij}
    $$
    根据期望的线性性, $\Ep{X}=\Ep{\sum_{i=1}^{n-1}\sum_{j=i+1}^n X_{ij}}=\sum_{i=1}^{n-1}\sum_{j=i+1}^n \Ep{X_{ij}}$. 

    由于$X_{ij}$只能取0和1, 是指示变量, $\Ep{X_{ij}}$是$X_{ij}$等于1的概率. 

    什么时候$y_i$和$y_j$会发生比较呢? 我们发现$y_i$和$y_j$发生比较, 当且仅当$y_i$或$y_j$是从集合$Y_{ij}=\{y_i, y_{i+1}, \cdots, y_{j-1},y_j\}$中选取的一个基准元素. 否则, 他们会被分在不同的子列表中, 因而不会比较. 

    由于我们的基准元素是独立选取的, 因此$y_i$和$y_j$是从$Y_{ij}$中选取的一个基准元素的概率, 也就是$X_{ij}$取1的概率, 是$2/(j-i+1)$. 也就是
    $$
    \begin{aligned}
        \Ep{X}&=\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} \frac 2{j-i+1}\\
        &\varsub{k:=j-i+1}{1.5cm} \sum_{i=1}^{n-1}\sum_{k=2}^{n-i+1} \frac 2k = \sum_{k=2}^n \sum_{i=1}^{n+1-k} \frac 2k \\ 
        &= \sum_{k=2}^n (n+1-k) \frac 2k = \left((n+1)\sum_{k=2}^n \frac 2k\right)-2(n-1) \\
        &= (2n+2) \sum_{k=1}^n \frac 1k - 4n.
    \end{aligned} 
    $$
\end{proof}