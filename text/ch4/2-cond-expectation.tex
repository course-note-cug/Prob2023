\section{条件数学期望}



    \begin{definition}
        设随机变量$X$在$y=y_i$的条件下的条件分布列为
        $$
        p_{i|j}=P(X=x_i | Y=y_j),
        $$
        若级数$\sum_i x_i p_{i|j}$绝对收敛, 那么称此级数$Y=Y_i$条件下$X$的条件数学期望, 记为$\Ep{X|Y=y_i}$.
    \end{definition}

    正如条件概率也是概率, 条件数学期望也是数学期望. 自然, 它就满足数学期望对应的性质. 

    \begin{example}
        有一根长度为$l$的棍子, 在$(0,l)$内均匀地取一点$X$折断. 再次在$(0,a)$选一点$Y$折断. 求$Y$的期望. 
    \end{example}

    \begin{solution}
        假设$Y$是一个具体值的情形. $\Ep{X \mid Y=y}=\frac{y}{2}$是一个数字. 但是实验之前, 这个值是随机的. 我们干脆把它写作$\Ep{X|Y}$作为一个随机变量. 其中, 记作$\Ep{X|Y}=Y/2$.
    \end{solution}



    下面来看有时候如何方便地计算期望. 我们可以把样本空间构成一组不相交的集合$A_1, A_2, \cdots, A_n$, 因此可以在每个小块上面计算期望值. 也就是: $$
    \Ep{X}=P\left(A_1\right) \Ep{X \mid A_1}+\cdots+P\left(A_n\right) \Ep{X \mid A_n} .
    $$
    这个可以使用全概率公式, 两边乘上其对应的系数得到. 这就是\textbf{全期望公式}. 


    有了这样的记号, 我们给出指数分布的期望的又一说明: 
    \begin{example}
        
    我们把原来的事件划分为不相交的两类. 
    $$
    \begin{aligned}
    & A_1:\{x=1\}, \quad A_2:\{x>1\} . \\
    & \Ep{X}=P(X=1) \Ep{X \mid x=1}+P(X>1) \Ep{X \mid x>1} .
    \end{aligned}
    $$
    那么计算它就得到了: 
    $$
\begin{aligned}
\Ep{X} & =P(X=1) E[X \mid x=1]+P(X>1) E[X \mid x>1] . \\
& =p \cdot 1+(1-p)\boxed{~~?~~}
\end{aligned}
$$
我们关注$E[X \mid x>1]$的情形, $$
\begin{aligned}
E[X \mid x-1>0] & =E[X-1 \mid x-1>0]+1 \\
& =\Ep{X}+1
\end{aligned}
$$
由于指数分布的无记忆性, 上面的式子就是$\Ep{X}+1$. 

上面的式子经过整理得到$\Ep{X}=p \cdot 1+(1-p)(\Ep{X}+1)$. 我们由此解答出来$\Ep{X}$, 因此就得到期望值为$1/p$. 

    \end{example}

    \subsection{迭代的期望}

    我们接下来考虑期望的迭代公式, 也就是$\mathbb{E}[\mathbb{E}[X \mid Y]]$. 这个符号看上去很难懂. 我们先从简单来看这个公式: 
    \begin{itemize}
        \item $\mathbb{E}[X \mid Y=y]$表示当随机变量$Y$取值$y$的时候, 随机变量$X$的期望. 
        \item 考虑所有可能的$Y$, 其构成的一组期望就是$\mathbb{E}_y[X \mid Y=y], \forall y$. 
        \item 这一组期望, 是一个关于$Y$的随机变量.为了简便,我们把这样的一组简写为$\mathbb{E}[X \mid Y]$.
        \item 对于这样的一个随机变量, 自然可以求它的期望. 
    \end{itemize}

    实际上, 这个公式有一个更简洁的表示. 我们按照定义展开: 
    $$\begin{aligned} \text{左手边} & =\int_{-\infty}^{+\infty}\left(\int_{-\infty}^{+\infty} \frac{f_{x, Y}(x, y)}{f_Y(y)} d x\right) f_Y(y) d y \\ & =\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} x f_{x, Y}(x, y) d x d y . \\ & =\int_{-\infty}^{+\infty} x f_x(x) d x=\mathbb{E}[x]=\text{右手边}\end{aligned}$$

    这个公式有一个名字, 叫做``迭代期望定律(Law of iterated expectations)''. 

    \begin{theorem}
        迭代的期望公式:$$
\mathbb{E}[{\mathbb{E}}[X \mid Y]]=\mathbb{E}[X]
$$
    \end{theorem}

    \subsection{练习: 仅仅依赖上一个状态的Markov链}

        \begin{wrapfigure}{l}{0.4\textwidth}
        \begin{center}
            \input{fig/ch4/markov-eg.tex}
        \end{center}
        \caption{班主任探访的规律}
        \label{fig:meeting-supervisor-pattern}
    \end{wrapfigure}
    \paragraph{第一幕 Markov链带来的稳定分布} 有三个喜欢自学的好奇学生A,B,C. 他们整天喜欢在宿舍学习. 这自然会使他们的班主任不开心. 这就会导致班主任每天就去他们的寝室去观察, 以确保他们确实在学习. 
    
    由于他们足够观察的足够仔细, 经过一段时间的观察, 他们认为班主任会按照如\cref{fig:meeting-supervisor-pattern}这样的的规律来``探访''他们.  如果我们只看$A$同学和黑色的箭头, 其意义如下: 

    若今天班主任找$A$同学, 那么明天: 
    \begin{itemize}
        \item 0.6的概率找$B$;
        \item 0.2的概率找$C$;
        \item 0.2的概率找$A$.
    \end{itemize}

    这里一个关键的假设是: 未来的状态仅仅取决于\emph{当前状态}. 也就是$P(X_{n+1}=x|X_n=x_n)$.
    
    在这个假设下, 如果我们已知班主任老师第一天找$B$, 第二天找$A$, 那么请问, 第三天找$C$的概率为多大? 按照我们刚刚的假设, 第三天找谁只与第二天有关. 因此第二天找了$A$, 自然第三天找$C$的概率为0.2. 

    自然, 日子一天天的过去, 他们想知道从长期来看, 班主任来``探访''他们的概率是多少. 比如, 他们记录了10天以内班主任的行踪轨迹: 
    $$
    A \to B \to A\to C \to A \to C\to C\to C\to A\to B.
    $$

    在这10天里面, $P(A\text{被找到}), P(B\text{被找到}),P(C\text{被找到})$的概率分别为4/10, 2/10, 4/10. 因此, 他们面临的第一个问题就是在前$n$天($n\to \infty$)的时候期望的结果是多少. 

    事实上, 上面的状态转换关系是一个图. 可以用$3\times 3$的矩阵表示如下(第一行和第一列为标示,$M[C][B]$表示第一天老师在$C$, 第二天去$B$的概率.):   
    $$
M=\left[\begin{array}{cccc}
&\text { A } & \text {B} & \text {C} \\
\text{A}&0.2 & 0.6 & 0.2 \\
\text{B}&0.3 & 0 & 0.7 \\
\text{C}& 0.5 & 0 & 0.5
\end{array}\right]
$$以及最初的状态(比如找了B, 初始状态用$1\times 3$的矩阵表示为$
\pi_0=\left[\begin{array}{ccc}
\text{A} & \text{B} & \text {C} \\
0 & 1 & 0
\end{array}\right]
$)若达到了稳定状态, 意味着找到这样的一个$\pi$, 使得$\pi A=\pi$. 这就是矩阵的特征向量.在这个情况下, 这个稳定的$\pi$为$\pi=\left[\frac{25}{71}, \frac{15}{71}, \frac{31}{71}\right]$.

为什么这样是对的呢? 我们考虑到达了状态$j$,从$i$状态开始,在 $n$ 步后, 定义为$P_{ij}(n)$. 同样如果我们有一个矩阵$$
M=\left[\begin{array}{ccc}
A_{11} & A_{12} & A_{13} \\ 
A_{21} & A_{22} & A_{23} \\ 
A_{31} & A_{32} & A_{33} \\ 
\end{array}\right]
$$
那么用一步从0到达2只有一种可能的情况:$P_{02}(1)=A_{02}$.  两步从0到2到达的一种情况就要分情况了. 
$$
\begin{aligned}
 P_{02}(2)&=A_{01} \times A_{12}+A_{00} \times A_{02}+A_{02} \times A_{22} \\
=& \left[\begin{array}{lll}
A_{00} & A_{01} & A_{02}
\end{array}\right] \times\left[\begin{array}{l}
A_{02} \\
A_{12} \\
A_{22}
\end{array}\right] \\
&
\end{aligned}
$$
类似的, 有$$
P_{10}(2)=\left[\begin{array}{lll}
A_{10} & A_{11} & A_{12}
\end{array}\right] \times\left[\begin{array}{c}
A_{00} \\
A_{10} \\
A_{20}
\end{array}\right]
$$
把每个状态都列举一遍, 我们就可以使用矩阵乘法. 也就是$$
P(2)=A^2=\left[\begin{array}{ccc}
0.5 & 0.2 & 0.3 \\
0.6 & 0.2 & 0.2 \\
0.1 & 0.8 & 0.1
\end{array}\right] \times\left[\begin{array}{ccc}
0.5 & 0.2 & 0.3 \\
0.6 & 0.2 & 0.2 \\
0.1 & 0.8 & 0.1
\end{array}\right]
$$也就是$P_{i_j}(n)=A_{i j}^n$. 

\paragraph{第二幕 Markov链的期望时间} 现在, 他们摸清楚了每一次他们老师来的模式. 
现在他们的想法是...