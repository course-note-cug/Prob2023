\section{Markov链对状态的分类}

我们发现很多时候Markov的状态转移的概率会给我们的最终状态研究产生影响. 例如, 如果Markov链进入了一个状态却无法出来, 那么这就会给我们带来一些线索. 基于此, 我们给出一些定义: 

\begin{definition}
    我们说从$i$到$j$是可达的, 如果存在某个整数$n\geq 0,P_{i,j}^n>0$. 如果两个状态$i,j$互相是可达的, 我们就说他们强联通, 写作$i \leftrightarrow j$. 
\end{definition}

实际上, 强联通是一个等价关系(我们在《离散数学》课上证明过了)\mn{离散数学! 又见面了. }. 也就是它具有如下的三条性质
\begin{itemize}
    \item [1.] 自反性: 对于任意的状态$i$, $i\leftrightarrow i$.
    \item [2.] 对称性: 如果$i\leftrightarrow j, $那么$j\leftrightarrow i$.
    \item [3.] 传递性: 如果$i\leftrightarrow j, j\leftrightarrow k$, 那么$i\leftrightarrow k$. 
\end{itemize}

有了这三条性质, 我们就可以进行分类了. 即: 可以把它们分为不相交的等价类, 使得每个等价类里面的状态是互相可达的, 但不同类别之间的状态是不可达的. 这些不相交的部分我们叫做``互达组''. 

特别的, 如果所有的节点属于同一个可达组的话, 我们就说这个Markov链是不可约的. 

\begin{definition}
    一个Markov链是不可约的, 如果所有的状态都属于同一个可达组. 
\end{definition}

这也就意味着, 如果我们的Markov链的任何一个状态经过若干步之后能够到达这个链的其他任意的一个状态, 那么我们就称他为不可约的Markov链. 并且我们发现, 如果Markov链状态数量有限, 我们只有在这个图是强联通的时候才能得到一个不可约的Markov链. 

\begin{lemma}
    当且仅当一个有限Markov链的图表示是强连通图(任何两个点都可以互相到达)时, 它是不可约的. 
\end{lemma}

接下来我们区分状态之间的一些特性. 为了做到这一点, 我们希望刻画从第一次$i$走到$j$的时候, 花了多少时间. 并且根据此分类. 

\begin{definition}
    我们把假设现在在状态$i$, 第一次转移到$j$的时间为$t$的概率记作$r_{i, j}^t$. 也就是
    $$
    r_{i, j}^t=P\left(X_t=j \text{, 并且对于} 1 \leq s \leq t-1, X_s \neq j \mid X_0=i\right).
    $$
\end{definition}

\begin{definition}
    如果$\sum_{t \geq 1} r_{i, i}^t=1$, 那么这个状态是常返的; 如果$\sum_{t \geq 1} r_{i, i}^t<1$, 我们就说这个状态是瞬时的(也称为非常返的). 
\end{definition}

直观上面来看, 如果一个状态是常返的, 那就意味着到最后这个链会反复地, 无限地访问这个状态. 反之, 如果这个是瞬时的, 意味着从这个状态$i$出发, 有一个固定的概率$p=\sum_{t \geq 1} r_{i, i}^t$返回这个状态$i$. 而且有一定的概率永远不会返回这个节点. 

我们把从$i$开始, 第一次回到$i$的时间叫做``从$i$到$i$的击中时间'', 那么这个随机变量我们就用``期望击中时间'', 用$h_{i,i}$表示, 就是$h_{i, i}:=\sum_{t \geq 1} t \cdot r_{i, i}^t$.  类似的, 我们给出如下的定义: 
\begin{definition}
    从$i$到$j$的``期望击中时间''指的是从$i$开始, 第一次回到$j$的时间这个随机变量的期望. 也就是
    $$
    h_{i,j}=\sum_{t \geq 1} t \cdot r_{i, j}^t
    $$
\end{definition}

我们发现如果一个链是常返的, 那么我们可以一直不停地访问某个链, 因此这个期望一定不会有限. 

\begin{example}
    考虑一个状态为非负整数的Markov链. 从$i$走向第$i+1$的概率是$i/(i+1)$, 从$i$走向$1$的概率为$i/(i+1)$. 

    那么, 从状态$1$开始, 在前 $t$ 步内未返回状态 $1$ 的概率是
    $$
    \prod_{j=1}^t \frac{j}{j+1}=\frac{1}{t+1}
    $$
    这就表明最后永远不回到状态$1$的概率为$0$. 因此, 前$t$步不返回, 第$t+1$步返回, 就有
    $$
    r_{1,1}^{t+1}=\frac{1}{(t+1)(t+1)}
    $$
    期望的击中时间就是: 
    $$
    h_{1,1}=\sum_{t=1}^{\infty} t \cdot r_{1,1}^t=\sum_{t=1}^{\infty} \frac{t}{(t+1)^2}
    $$
    是无界的. 
\end{example}

像这样, 我们把根据期望击中时间的存在性, 把状态这样分类. 

\begin{definition}
    一个常返状态$i$称为正(positive)常返, 当$h_{i, i}<\infty$. 否则, 称为零(null)常返. 
\end{definition}

实际上, 对于任何一个有限的的Markov链, 我们都会发现这样的一个性质: 

\begin{theorem}
    \label{thm:finite-prop}
    在一个有限的Markov链中, 
    \begin{itemize}
        \item 至少有一个状态是常返的, 
        \item 所有的常返状态都是正常返的. 
    \end{itemize}    
\end{theorem}

\begin{proof}
    使用反证法: 假设有一个有限状态$(0,1,2,\cdots, n)$的Markov链, 如果它的所有状态均为瞬时的, 那么经过有限时间$T_0$之后不能访问状态0, 有限时间$T_1$之后不能访问状态1, 以此类推,有限时间$T_n$之后不能访问状态$n$. 但是根据Markov链的性质, 经过充分长的有限时间之后一定还要在一个状态. 因此产生了矛盾. 
\end{proof}

有时候我们的Markov链会呈现规律的周期性. 我们来考察这个例子: 

\begin{example}
    考察状态为$\{1,2,\cdots\}$的正整数的Markov链. 当我们在状态$i$的时候, 我们有$1/2$的概率移动到状态$i+1$, 有$1/2$的概率移动到状态$i-1$. 如果我们从0号开始, 那么在奇数时间的时候, 我们只能发现当前状态在奇数的状态上面. 这就是周期性的一个体现. 
\end{example}

\begin{definition}
    在Markov链中, 一个状态$j$是周期的, 如果存在一个整数周期$\Delta>1$. 对于任意的$s$, 在$s$不整除$\Delta$的时候, $P\left(X_{t+s}=j \mid X_t=j\right)=0$. 

    在一个Markov链中, 如果每一个状态都是具有周期性的, 那么这个链就是具有周期性的. 否则, 称为没有周期性的. 
\end{definition}

所以, 上面的例子是一个周期为2的Markov链. 

最后我们来讨论在有限的Markov链的一个重要的属性. 如果给定了足够的时间，它们会访问所有的状态, 那这个状态该多好啊! 我们把这样的状态叫做``经历各种状态的(ergodic)'', 或者更简单地说: ``遍历的''. 我们可以用以前说过的定义缝合一个这样的定义: 

\begin{definition}
    Markov链中, 非周期性的正常返状态是遍历的状态. 如果所有的状态都是遍历的, 那么这个链是遍历的. 
\end{definition}

自然, 对于某些有限的Markov链, 我们都会发现整个链式遍历的. 

\begin{theorem}
    任何有限的, 不可约的, 非周期的Markov链是遍历的. 
\end{theorem}

\begin{proof}
    根据\cref{thm:finite-prop}我们知道任何一个有限的Markov链必须有一个常返态. 如果一个链是不可约的, 那么它的所有的状态都是常返的. 并且是正常返的. 因此, 这个链是没有周期的. 故是遍历的. 
\end{proof}


\begin{example}
    下面来看一个经典的问题: 赌徒破产问题. 考虑两人进行独立, 公平的赌博. 每一轮一个人以$1/2$的概率赢得一元, 以$1/2$的概率输掉一元. 在时刻$t$的状态是第一个人赢的钱数. 初始状态是0. 
    
    如果我们存在两个数$l_1, l_2$. 使得选手$i$不会输掉多于$l_i$元, 因此到达了$l_1, l_2$两个状态的时候, 游戏结束. 注意到$-l_1, l_2$两个状态是常返的, 其他的状态是瞬时的. 我们对于它最后会到达哪个状态感兴趣. 如果它最后以进入状态$i$结束, 我们称这个链被状态$i$ ``吸收''. 
    
    对于赌徒1而言, 他自然希望知道在输掉$l_1$元之前赢得$l_2$元的概率. 假设$P_i^t$是经过$t$步链处于状态$i$的概率. 因此$\lim _{t \rightarrow \infty} P_i^t=0$. 设$q$是以选手1赢得$l_2$元结束. 也就是说这个链最后被$l_2$吸收. 那么, $1-q$就是被$-l_1$吸收. 根据定义, $\lim _{t \rightarrow \infty} P_{l_2}^t=q$.

    由于这个赌博游戏的每次是公平且相互独立的, 因此参与者1在走任何一步之后的期望收益为0. 设$W_t$是$t$步后选手1的收益, 根据归纳法, 对于任意$t$, $\mathbb{E}\left[W^t\right]=0$, 也就是
    $$\mathbb{E}\left[W^t\right]=\sum_{i=-l_1}^{l_2} i P_i^t=0.$$
    并且
    $$
\begin{aligned}
\lim _{t \rightarrow \infty} \mathbb{E}\left[W^t\right] & =l_2 q-l_1(1-q) \\
& =0 .
\end{aligned}
$$
因此
$$
q=\frac{l_1}{l_1+l_2}
$$
这个结论展示了赢或输的概率与他们愿意赢/输的钱数成比例. 

我们可以使用另一种方式得到这个答案. 令$q_j$表示在选手1在已经获得$l_2$元时, 当$-l_1\leq j\leq l_2$在输掉$l_1$元前赢得$j$元的概率. 显然$q_{l_1}=0,q_{l_2}=1. $ 考虑第一次游戏的结果, 计算$q_j=q_{j-1}/2 + q_{j+1}/2$. 这样, 我们有$l_1+l_2-2$个线性无关的方程, 同时也有$l_1+l_2-2$个未知数. 因此一定有唯一解. 

\end{example}